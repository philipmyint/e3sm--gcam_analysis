import json
import multiprocessing
import numpy as np
import pandas as pd
import sys
import time
from utility_constants import *
from utility_dataframes import move_columns_next_to_each_other_in_dataframe, write_dataframe_to_fwf
from utility_functions import *
from utility_E3SM_netcdf import *

def extract_dataframe_rows_for_given_year(df, year):
    """ 
    Processes the Pandas DataFrame so that it contains only the rows pertaining to the given year, where the year (or 'time') is a DataFrame index.

    Parameters:
        df: DataFrame of interest.
        year: Year of interest.

    Returns:
        DataFrame containing only the rows for the given year, with the 'time' column dropped.
    """
    df = df.reset_index(['time'])
    df = df[df['time'] == year].drop(columns=['time'])
    return df

def extract_netcdf_file_into_dataframe_single_year(file, variables, region, year):
    """ 
    For the given year, extracts the specified variables from the E3SM human component (EHC) land surface data file into a Pandas DataFrame. 

    Parameters:
        file: NetCDF file generated by the EHC during run time. It contains data on land surface variables like harvest and grazing. 
        variables: List of variables we want to extract from the NetCDF file. 
        region: String for the region of interest. If not specified or not recognized, then there will be no restrictions on the lat/lon coordinates. 
        year: Year of interest.

    Returns:
        DataFrame containing one column for each of the variables, plus a column for the specific year of interest.
    """
    # Extract the area as a function of lat/lon coordinate from the NetCDF file and forms an xarray Dataset for the variables.
    areas, ds, _, _ = find_gridcell_areas_in_netcdf_file(file, region=region)

    # Drop duplicate 'time' coordinate values (e.g., that get generated during restarts), keeping the data that correspond to the last occurrence.
    ds = ds.drop_duplicates(dim='time', keep='last')

    # If grazing or harvest are of interest, make sure to also extract the land fractions for vegetation and the plant-functional types (PFTs).
    if 'GRAZING' in variables or any('HARVEST_' in variable for variable in variables):
        if 'PCT_NAT_PFT' not in variables:
            variables.append('PCT_NAT_PFT')
        if 'PCT_NATVEG' not in variables:
            variables.append('PCT_NATVEG')        

    # Convert the Dataset to create an overall DataFrame that stores all variables except those for land fractions and PFTs. 
    # Create a second DataFrame to store the land fractions and PFTs. 
    variables_except_pfts = variables.copy()
    variables_pfts = []
    if 'PCT_NAT_PFT' in variables:
        variables_except_pfts.remove('PCT_NAT_PFT')
        variables_pfts.append('PCT_NAT_PFT')
    if variables_except_pfts:
        df = ds[variables_except_pfts].to_dataframe()
        # Reduce the DataFrame to only the rows for the specific year of interest.
        df = extract_dataframe_rows_for_given_year(df, year)
    if variables_pfts:
        df_pfts = ds[variables_pfts].to_dataframe()
        df_pfts = extract_dataframe_rows_for_given_year(df_pfts, year)

    # Divide the vegetation percent by 100 to change it to a fraction and update the label accordingly.
    if 'PCT_NATVEG' in variables:
        df['PCT_NATVEG'] = df['PCT_NATVEG'].fillna(0)/100
        df = df.rename(columns={'PCT_NATVEG': 'FRAC_VEG'})

    # Add a column for the area at each lat/lon coordinate to the overall DataFrame.
    df['AREA (km^2)'] = areas

    # 17 PFTs in order: 1 bare, 8 tree, 3 shrub, 3 grass, 1 crop, 1 empty. These can be further subgrouped as follows:
    # Bare soil (index 0), forest (the 8 trees, indices 1--8); shrub (indices 9--11); grass (indices 12--14), crop (index 15). Ignore the empty PFT.
    if 'PCT_NAT_PFT' in variables:
        df_pfts = df_pfts.reset_index(level='natpft')
        # Get data for the individual PFTs (again ignoring the empty one), as well as the aggregate subgroups.
        pft_labels = [f'PFT_{i+1}_AREA (km^2)' for i in range(16)]
        pft_labels.extend(['BARE_AREA (km^2)', 'FOREST_AREA (km^2)', 'SHRUB_AREA (km^2)', 'GRASS_AREA (km^2)', 'CROP_AREA (km^2)'])
        pft_min_max_indices = [(i, i) for i in range(16)]
        pft_min_max_indices.extend([(0, 0), (1, 8), (9, 11), (12, 14), (15, 15)])
        # Select only the rows that pertain to this particular PFT category and for each lat/lon coordinate, sum over all PFTS if a subgroup.
        for index, pft_label in enumerate(pft_labels):
            pft_min_index, pft_max_index = pft_min_max_indices[index][0], pft_min_max_indices[index][1]
            df_this_pft = df_pfts[(df_pfts['natpft'] >= pft_min_index) & (df_pfts['natpft'] <= pft_max_index)]
            # Add up the percentages over all PFTs in the subgroup and divide that total percent by 100 to change it to a fraction.
            df_this_pft = df_this_pft['PCT_NAT_PFT'].groupby(['lsmlat', 'lsmlon']).sum().fillna(0)/100
            # Add a column to the overall DataFrame to record the area of this PFT category (individual or subgroup) at each lat/lon coordinate.
            df[pft_label] = df['AREA (km^2)']*df['FRAC_VEG']*df_this_pft      

    # Convert the grazing and harvest variables from a unitless fraction into an area by multiplying with the area of the PFT aggregate subgroup.
    grazing_harvest_variables = ['GRAZING', 'HARVEST_SH1', 'HARVEST_SH2', 'HARVEST_SH3', 'HARVEST_VH1', 'HARVEST_VH2']
    pft_area_columns = ['GRASS_AREA (km^2)', 'FOREST_AREA (km^2)', 'FOREST_AREA (km^2)', 'FOREST_AREA (km^2)', 'FOREST_AREA (km^2)', 
                          'FOREST_AREA (km^2)']
    for index in range(len(grazing_harvest_variables)):
        grazing_harvest_variable = grazing_harvest_variables[index]
        df[grazing_harvest_variable] = df[grazing_harvest_variable]*df[pft_area_columns[index]]
        df = df.rename(columns={grazing_harvest_variable: grazing_harvest_variable + '_AREA (km^2)'})
    
    # Create another column for the total harvest, which is the sum of all the harvest variables present in the DataFrame.
    harvest_variables = ['HARVEST_SH1_AREA (km^2)', 'HARVEST_SH2_AREA (km^2)', 'HARVEST_SH3_AREA (km^2)', 'HARVEST_VH1_AREA (km^2)', 
                         'HARVEST_VH2_AREA (km^2)']
    harvest_variables = [x for x in harvest_variables if x in df.columns]
    if harvest_variables:
        df['HARVEST_AREA (km^2)'] = df[harvest_variables].sum(axis=1)
        df = move_columns_next_to_each_other_in_dataframe(df, harvest_variables[len(harvest_variables)-1], 'HARVEST_AREA (km^2)')
    
    # Sum over all latitude/longitude coordinates to get an area-weighted sum for each variable.
    df['FRAC_VEG'] *= df['AREA (km^2)']
    df = df.sum().to_frame().T

    # Vegetation fraction should be an area-weighted mean, and not an area-weighted sum, so we should divide its sum by the total area.
    df['FRAC_VEG'] /= np.sum(areas)

    # Add year column.
    column_names_with_year_first = ['Year']
    column_names_with_year_first.extend(df.columns)
    df['Year'] = year
    return df[column_names_with_year_first]

def extract_time_series_from_netcdf_file(inputs):
    """ 
    Extracts the specified variables from an E3SM-generated NetCDF file into a Pandas DataFrame and performs the indicated operation on the variables.
    This NetCDF file is generated dynamically during run time by the E3SM human component (EHC) and contains land surface data like harvest, grazing. 

    Parameters:
        inputs: Dictionary containing the user choice inputs for different options, such as the variables that they want to extract from the file. 

    Returns:
        DataFrame containing one column for each of the variables, plus a column for the year.
    """
    # Extract all user selections, some being supplied with default choices. Calculate the total number of years. Set the path and name of the file.
    simulation_path = inputs['simulation_path']
    output_file = inputs['output_file']
    variables = inputs['variables']
    region = inputs.get('region', None)
    write_to_csv = inputs.get('write_to_csv', False)
    start_year = inputs.get('start_year', 2015)
    end_year = inputs.get('end_year', 2100)
    num_years = end_year - start_year + 1
    file = os.path.join(simulation_path, 'surfdata_iESM_dyn.nc')

    # Use multiprocessing to extract data from the file for each individual year. 
    # Put the data from each year into DataFrame and store all such DataFrames in a list.
    arguments = list(zip([file]*num_years, [variables]*num_years, [region]*num_years, range(start_year, end_year+1)))
    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:
        dataframes_for_each_year = list(pool.starmap(extract_netcdf_file_into_dataframe_single_year, arguments))

    # Concatenate all DataFrames in the list together to form a single DataFrame over all years. Sort by year.
    df = pd.concat(dataframes_for_each_year)
    df.sort_values(['Year'], inplace=True)  

    # Write the DataFrame to the specified output file.
    if write_to_csv or output_file.endswith('.csv'):
        df.to_csv(output_file, index=False)
    else:
        write_dataframe_to_fwf(output_file, df)


###---------------Begin execution---------------###
if __name__ == '__main__':

    # Run this script together with the input JSON file(s) on the command line.
    start_time_total = time.time()
    if len(sys.argv) < 2:
        print('Usage: plot_spatial_data.py `path/to/json/input/file(s)\'')
        sys.exit()

    # Read and load the JSON file(s) into a list of dictionaries. Each block in a JSON file represents one time series output file.
    list_of_inputs = []
    for index in range(1, len(sys.argv)):
        input_file = sys.argv[index]
        with open(input_file) as f:
            list_of_inputs.extend(json.load(f))

    # Produce the output files one at a time.
    for inputs in list_of_inputs:
        start_time = time.time()
        extract_time_series_from_netcdf_file(inputs)
        end_time = time.time()
        elapsed_time = end_time - start_time
        print(f"Elapsed time for {inputs['output_file']}: {elapsed_time:.2f} seconds")
    
    # Print the total execution time needed to complete all data extraction operations.
    end_time = time.time()
    elapsed_time = end_time - start_time_total
    print(f"Elapsed time for extracting all time series data: {elapsed_time:.2f} seconds")
